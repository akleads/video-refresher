---
phase: 08-download-cleanup-job-lifecycle
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/package.json
  - server/routes/jobs.js
  - server/lib/processor.js
autonomous: true

must_haves:
  truths:
    - "GET /api/jobs/:id/download returns a ZIP file with all variations organized into folders by source video name"
    - "ZIP uses STORE compression (no re-compression of H.264 video data)"
    - "Download returns 404 for non-completed jobs and 410 for expired jobs"
    - "Upload source files are deleted from the volume after processing completes"
  artifacts:
    - path: "server/routes/jobs.js"
      provides: "GET /:id/download endpoint with streaming ZIP"
      contains: "archiver.*store.*true"
    - path: "server/lib/processor.js"
      provides: "Upload file cleanup after processJob completes"
      contains: "unlinkSync"
  key_links:
    - from: "server/routes/jobs.js"
      to: "archiver"
      via: "import and archive.pipe(res)"
      pattern: "archiver.*zip.*store"
    - from: "server/lib/processor.js"
      to: "job_files.upload_path"
      via: "fs.unlinkSync after all files processed"
      pattern: "unlinkSync.*upload_path"
---

<objective>
Add streaming ZIP download endpoint and upload file cleanup after processing.

Purpose: Users need to download all processed variations as a single ZIP, and upload source files should be cleaned up immediately after processing to save volume space.
Output: Working GET /api/jobs/:id/download endpoint that streams a STORE-compressed ZIP organized by source video name, plus automatic upload file deletion after processJob completes.
</objective>

<execution_context>
@/Users/alexkozhemiachenko/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alexkozhemiachenko/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-download-cleanup-job-lifecycle/08-RESEARCH.md

@server/package.json
@server/routes/jobs.js
@server/lib/processor.js
@server/db/queries.js
@server/db/schema.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install archiver and add ZIP download endpoint</name>
  <files>server/package.json, server/routes/jobs.js</files>
  <action>
    1. Install archiver: `cd server && npm install archiver`

    2. In `server/routes/jobs.js`, add the download endpoint BEFORE the `return router` line:

    Import archiver and path at the top:
    ```
    import archiver from 'archiver';
    import path from 'node:path';
    ```

    Add endpoint `GET /:id/download` with `requireAuth`:
    - Get job via `queries.getJob.get(req.params.id)`
    - Return 404 if job not found or status is not 'completed'
    - Check expiry: parse `job.expires_at` (append 'Z' for UTC) and compare to `new Date()`. Return 410 `{ error: 'Job expired' }` if past expiry.
    - Get `outputFiles = queries.getOutputFiles.all(job.id)` and `jobFiles = queries.getJobFiles.all(job.id)`
    - If outputFiles is empty, return 404 `{ error: 'No output files available' }`
    - Build a Map from job_file_id to folder name: `jf.original_name.replace(/\.mp4$/i, '')`
    - Set response headers:
      - `Content-Type: application/zip`
      - `Content-Disposition: attachment; filename="video-refresher-${job.id}.zip"`
    - Create archive: `archiver('zip', { store: true })` -- use `{ store: true }` specifically, NOT `{ zlib: { level: 0 } }` (the latter does NOT produce STORE-method ZIPs, it still wraps in DEFLATE headers)
    - Register error handler on archive: log error, destroy response if headers already sent
    - Register warning handler: log ENOENT warnings (missing files)
    - Pipe archive to response: `archive.pipe(res)`
    - Loop over outputFiles, for each: `archive.file(out.output_path, { name: folderName + '/' + path.basename(out.output_path) })`
    - Call `archive.finalize()` -- this is critical, without it the ZIP stream never ends
  </action>
  <verify>
    Run `cd /Users/alexkozhemiachenko/Downloads/Claude/video-refresher/server && node -e "import('archiver').then(m => console.log('archiver OK'))"` to verify the dependency is installed.
    Run `node -e "import('./routes/jobs.js').then(m => console.log('jobs.js imports OK'))"` from the server directory to verify the module parses without errors.
  </verify>
  <done>
    - archiver is in package.json dependencies
    - GET /:id/download endpoint exists in jobs.js
    - Endpoint uses `archiver('zip', { store: true })` (not zlib level 0)
    - Endpoint checks auth, job existence, completion status, and expiry
    - ZIP entries are organized as `{original_name_without_ext}/{variation_filename}`
    - archive.finalize() is called after all files are added
  </done>
</task>

<task type="auto">
  <name>Task 2: Add upload file cleanup after processing</name>
  <files>server/lib/processor.js</files>
  <action>
    In `server/lib/processor.js`, add upload file cleanup at the END of the `processJob` function, AFTER the job status is set (after the if/else block at step 5).

    Import fs at the top if not already imported (it IS already imported -- `import fs from 'node:fs'`).

    After the final job status update (after the closing brace of the allFailed else block), add:

    ```javascript
    // 6. Clean up upload source files (best-effort)
    for (const file of files) {
      try {
        fs.unlinkSync(file.upload_path);
      } catch (err) {
        // Log but don't fail -- upload deletion is best-effort
        console.error(`Failed to delete upload ${file.upload_path}:`, err.message);
      }
    }
    ```

    This runs for ALL job outcomes (completed, partial success, all failed) because upload files are never needed after processing finishes. The `files` array is already loaded at the top of processJob (step 2).
  </action>
  <verify>
    Run `node -e "import('./lib/processor.js').then(m => console.log('processor.js imports OK'))"` from the server directory to verify the module still parses.
    Visually verify the unlinkSync loop is positioned AFTER the job status update logic (step 5) and BEFORE the function ends.
  </verify>
  <done>
    - processJob deletes each file's upload_path after processing completes
    - Deletion is wrapped in try/catch (best-effort, does not fail the job)
    - Cleanup runs regardless of job outcome (completed, partial, or all-failed)
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/alexkozhemiachenko/Downloads/Claude/video-refresher/server && node -e "import('archiver').then(m => console.log('OK'))"` -- archiver loads
2. `node -e "import('./routes/jobs.js').then(m => console.log('OK'))"` -- jobs route parses
3. `node -e "import('./lib/processor.js').then(m => console.log('OK'))"` -- processor parses
4. Grep for `store: true` in jobs.js to confirm correct ZIP method
5. Grep for `unlinkSync` in processor.js to confirm upload cleanup
</verification>

<success_criteria>
- archiver ^7.0.1 installed in server/package.json
- GET /:id/download streams a STORE-compressed ZIP organized by source video folders
- Download returns 404 for missing/non-completed jobs, 410 for expired jobs
- Upload source files are deleted after processJob completes (all outcomes)
- All modules parse without import errors
</success_criteria>

<output>
After completion, create `.planning/phases/08-download-cleanup-job-lifecycle/08-01-SUMMARY.md`
</output>
