---
phase: 12-server-job-cancellation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/db/schema.js
  - server/db/queries.js
  - server/lib/ffmpeg.js
  - server/lib/cancel.js
  - server/lib/processor.js
  - server/lib/queue.js
  - server/lib/cleanup.js
  - server/routes/jobs.js
  - server/index.js
autonomous: true

must_haves:
  truths:
    - "POST /api/jobs/:id/cancel returns 200 and kills running FFmpeg for processing jobs"
    - "POST /api/jobs/:id/cancel returns 200 and instantly cancels queued jobs (no FFmpeg to kill)"
    - "FFmpeg termination uses 3-stage escalation: stdin 'q' -> 2s -> SIGTERM -> 2s -> SIGKILL"
    - "Completed variations are preserved after cancellation and remain downloadable"
    - "Partial (in-progress) output files are deleted after cancellation"
    - "Job status is 'cancelled' after cancel, unless all variations already completed (completion wins)"
    - "Cancelled jobs are included in cleanup daemon expiry (same 24h lifecycle)"
  artifacts:
    - path: "server/lib/cancel.js"
      provides: "cancelJob() function with 3-stage FFmpeg termination"
      exports: ["cancelJob"]
    - path: "server/routes/jobs.js"
      provides: "POST /:id/cancel endpoint"
    - path: "server/lib/processor.js"
      provides: "Cancellation check between variations"
      contains: "isCancelled"
    - path: "server/db/queries.js"
      provides: "Cancel-related prepared statements"
      contains: "cancelJob"
  key_links:
    - from: "server/routes/jobs.js"
      to: "server/lib/cancel.js"
      via: "cancelJob() import and call"
      pattern: "cancelJob"
    - from: "server/lib/cancel.js"
      to: "server/lib/ffmpeg.js"
      via: "process.kill() on tracked PID after stdin 'q'"
      pattern: "stdin.*write.*q"
    - from: "server/lib/processor.js"
      to: "server/db/queries.js"
      via: "isCancelled check between variations"
      pattern: "isCancelled|cancelled"
---

<objective>
Add server-side job cancellation: cancel endpoint, graceful 3-stage FFmpeg termination, cancellation-aware processing loop, and partial file cleanup.

Purpose: Users need to stop in-progress server jobs without losing already-completed variations. The server must gracefully kill FFmpeg processes (stdin 'q' -> SIGTERM -> SIGKILL) and clean up only partial output files.

Output: Working POST /api/jobs/:id/cancel endpoint that handles both queued and processing jobs, with database schema supporting 'cancelled' status and cancelled_at timestamp.
</objective>

<execution_context>
@/Users/alexkozhemiachenko/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alexkozhemiachenko/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-server-job-cancellation/12-CONTEXT.md
@.planning/phases/12-server-job-cancellation/12-RESEARCH.md

@server/db/schema.js
@server/db/queries.js
@server/lib/ffmpeg.js
@server/lib/processor.js
@server/lib/queue.js
@server/lib/cleanup.js
@server/routes/jobs.js
@server/index.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database schema, queries, and FFmpeg stdin + cancel function</name>
  <files>
    server/db/schema.js
    server/db/queries.js
    server/lib/ffmpeg.js
    server/lib/cancel.js
  </files>
  <action>
1. **server/db/schema.js** -- Add migration for `cancelled_at` column on `jobs` table:
   ```js
   { table: 'jobs', name: 'cancelled_at', sql: "ALTER TABLE jobs ADD COLUMN cancelled_at TEXT" }
   ```
   No need to add 'cancelled' as a status enum -- SQLite uses TEXT, so any string value works.

2. **server/db/queries.js** -- Add these prepared statements to createJobQueries():
   ```js
   // Cancel-related queries
   cancelJob: db.prepare(`
     UPDATE jobs SET status = 'cancelled', cancelled_at = datetime('now'), updated_at = datetime('now')
     WHERE id = ? AND status IN ('queued', 'processing')
   `),

   isJobCancelled: db.prepare(`
     SELECT 1 FROM jobs WHERE id = ? AND status = 'cancelled'
   `),
   ```
   Also update `getExpiredJobs` to include 'cancelled' status:
   ```js
   getExpiredJobs: db.prepare(`
     SELECT * FROM jobs
     WHERE expires_at < datetime('now')
       AND status IN ('completed', 'failed', 'cancelled')
   `),
   ```
   And update `getEvictionCandidates` similarly:
   ```js
   getEvictionCandidates: db.prepare(`
     SELECT * FROM jobs
     WHERE status IN ('completed', 'failed', 'cancelled')
     ORDER BY updated_at ASC
   `),
   ```

3. **server/lib/ffmpeg.js** -- Change `spawnFFmpeg` to pipe stdin instead of ignoring it. Change line 76 from:
   ```js
   const ffmpegChild = spawn('ffmpeg', args, { stdio: ['ignore', 'pipe', 'pipe'] });
   ```
   to:
   ```js
   const ffmpegChild = spawn('ffmpeg', args, { stdio: ['pipe', 'pipe', 'pipe'] });
   ```
   This enables writing 'q' to stdin for graceful termination. FFmpeg reads stdin for interactive commands; with 'ignore' the 'q' approach would fail.

4. **server/lib/cancel.js** -- Create new file. Export `cancelJob(pid)` function that implements 3-stage graceful termination:
   - Stage 1: Check if process exists with `process.kill(pid, 0)`. If ESRCH (no such process), return immediately (already dead).
   - Stage 2: Try writing 'q\n' to stdin. This requires access to the child process object, NOT just the PID. Since we only have the PID at cancel time (stored in DB), we cannot write to stdin of an arbitrary PID. **Instead, store the ChildProcess reference in a Map keyed by job file ID.** Export `registerProcess(fileId, childProcess)` and `unregisterProcess(fileId)`.
   - For the cancellation function `cancelJobProcesses(fileId)`:
     a. Look up ChildProcess from the Map by fileId.
     b. If found: write 'q\n' to childProcess.stdin, then set a 2-second timeout.
     c. After 2s, check if process still running with `process.kill(pid, 0)`. If still alive, send SIGTERM.
     d. After another 2s, check again. If still alive, send SIGKILL.
     e. If NOT found in Map (process already exited): try `process.kill(pid, 0)` to check if PID exists. If it does, go straight to SIGTERM -> 2s -> SIGKILL (no stdin available).
   - Return a Promise that resolves when the process is confirmed dead (or after SIGKILL).
   - Handle ESRCH errors gracefully at each stage (process already dead = success).
   - Export: `registerProcess`, `unregisterProcess`, `cancelJobProcesses`, and also `getActiveProcess` for testing.

   ```js
   import { setTimeout as delay } from 'node:timers/promises';

   // Map of fileId -> { process: ChildProcess, pid: number }
   const activeProcesses = new Map();

   export function registerProcess(fileId, childProcess) {
     activeProcesses.set(fileId, { process: childProcess, pid: childProcess.pid });
   }

   export function unregisterProcess(fileId) {
     activeProcesses.delete(fileId);
   }

   function isProcessAlive(pid) {
     try {
       process.kill(pid, 0);
       return true;
     } catch (err) {
       return false; // ESRCH = not running
     }
   }

   export async function cancelJobProcesses(fileId) {
     const entry = activeProcesses.get(fileId);

     if (entry && entry.process && !entry.process.killed) {
       const { process: child, pid } = entry;

       // Stage 1: stdin 'q'
       try {
         if (child.stdin && !child.stdin.destroyed) {
           child.stdin.write('q\n');
         }
       } catch (err) {
         // stdin may already be closed
       }

       await delay(2000);
       if (!isProcessAlive(pid)) return;

       // Stage 2: SIGTERM
       try { process.kill(pid, 'SIGTERM'); } catch (err) { return; }

       await delay(2000);
       if (!isProcessAlive(pid)) return;

       // Stage 3: SIGKILL
       try { process.kill(pid, 'SIGKILL'); } catch (err) { /* already dead */ }
       return;
     }

     // Fallback: no ChildProcess reference, try PID-based kill
     if (entry && entry.pid) {
       const pid = entry.pid;
       if (!isProcessAlive(pid)) return;

       try { process.kill(pid, 'SIGTERM'); } catch (err) { return; }
       await delay(2000);
       if (!isProcessAlive(pid)) return;

       try { process.kill(pid, 'SIGKILL'); } catch (err) { /* already dead */ }
     }
   }
   ```
  </action>
  <verify>
    Run `node -e "import('./server/lib/cancel.js').then(m => console.log(Object.keys(m)))"` from the project root to confirm the module exports registerProcess, unregisterProcess, cancelJobProcesses. Also run `node -e "import('./server/db/queries.js')"` -- if there are syntax errors it will fail.
  </verify>
  <done>
    - cancel.js exports registerProcess, unregisterProcess, cancelJobProcesses
    - queries.js has cancelJob and isJobCancelled prepared statements
    - schema.js adds cancelled_at migration
    - ffmpeg.js spawns with stdin piped
    - cleanup queries include 'cancelled' status
  </done>
</task>

<task type="auto">
  <name>Task 2: Cancel endpoint, processor cancellation loop, and queue wiring</name>
  <files>
    server/routes/jobs.js
    server/lib/processor.js
    server/lib/queue.js
    server/index.js
  </files>
  <action>
1. **server/lib/processor.js** -- Make the processing loop cancellation-aware and register/unregister FFmpeg processes.
   - Import `registerProcess` and `unregisterProcess` from `./cancel.js`.
   - In `processFile()`, after calling `spawnFFmpeg()` (line 58), call `registerProcess(file.id, ffmpegResult.process)` right after storing the PID (line 61).
   - After `await ffmpegResult.promise` (line 64), call `unregisterProcess(file.id)` right before clearing the PID (line 67).
   - Add a cancellation check BETWEEN variations. After `unregisterProcess(file.id)` and before the next loop iteration, check if the job has been cancelled:
     ```js
     // Check for cancellation between variations
     const cancelled = queries.isJobCancelled.get(job.id);
     if (cancelled) {
       throw new Error('Job cancelled');
     }
     ```
   - In `processJob()`, update the catch block for processFile to distinguish cancellation from regular errors:
     ```js
     } catch (err) {
       if (err.message === 'Job cancelled') {
         // Don't mark file as failed -- job was cancelled
         queries.updateFileStatus.run('cancelled', file.id);
         break; // Stop processing remaining files
       }
       allSucceeded = false;
       queries.updateFileError.run(err.message, file.id);
       console.error(`File ${file.id} (${file.original_name}) failed:`, err.message);
     }
     ```
   - After the file processing loop in `processJob()`, add a check: if the job status is already 'cancelled' (set by the cancel endpoint), do NOT overwrite it with 'completed' or 'failed'. Add this before the status update block:
     ```js
     // Re-check job status -- cancel endpoint may have already set it
     const currentJob = queries.getJob.get(job.id);
     if (currentJob.status === 'cancelled') {
       // Clean up partial output files (incomplete variations)
       cleanupPartialFiles(job.id, queries);
       // Still clean up uploads
       // (falls through to upload cleanup below)
     } else if (allSucceeded) {
       queries.updateJobStatus.run('completed', job.id);
     } else { ... }
     ```
   - Add a helper function `cleanupPartialFiles(jobId, queries)` at the top of processor.js:
     ```js
     function cleanupPartialFiles(jobId, queries) {
       // Get all output files for this job
       const outputFiles = queries.getOutputFiles.all(jobId);
       const jobFiles = queries.getJobFiles.all(jobId);

       // For each job_file, check if it was mid-processing
       // If a file's completed_variations < expected variations, delete the LAST output
       // (which may be partial/corrupt from the killed FFmpeg)
       for (const jf of jobFiles) {
         if (jf.status === 'processing' || jf.status === 'cancelled') {
           const outputs = outputFiles.filter(o => o.job_file_id === jf.id);
           // The last output might be partial if FFmpeg was killed mid-encode
           // Check if the file exists and is valid (> 0 bytes with moov atom)
           // Simpler approach: the variation that was being processed when killed
           // never gets insertOutputFile called (promise rejects), so no DB record
           // exists for it. The only partial file is on disk but NOT in DB.
           // We need to scan the output directory for files not in the DB.
         }
       }

       // Scan output directory for orphaned files
       const jobOutputDir = path.join(outputDir, jobId);
       if (fs.existsSync(jobOutputDir)) {
         const diskFiles = fs.readdirSync(jobOutputDir);
         const dbPaths = new Set(outputFiles.map(o => path.basename(o.output_path)));

         for (const diskFile of diskFiles) {
           if (!dbPaths.has(diskFile)) {
             // Orphaned file -- partial output from killed FFmpeg
             const orphanPath = path.join(jobOutputDir, diskFile);
             try {
               fs.unlinkSync(orphanPath);
               console.log(`Cleaned up partial file: ${orphanPath}`);
             } catch (err) {
               console.error(`Failed to clean up partial file ${orphanPath}:`, err.message);
             }
           }
         }
       }
     }
     ```
   NOTE: `cleanupPartialFiles` needs access to `outputDir`. Pass it as a parameter or use the one already available in processJob's scope. Since processJob receives `outputDir`, pass it to cleanupPartialFiles: `cleanupPartialFiles(job.id, queries, outputDir)`.
   - Also make the function handle the case where FFmpeg's promise rejects due to being killed (non-zero exit code). The existing promise reject in ffmpeg.js will throw, which is caught by processFile's caller. The 'q' command causes FFmpeg to exit with code 0 (clean quit). SIGTERM causes non-zero. Wrap the ffmpegResult.promise await in a try-catch that checks for cancellation:
     ```js
     try {
       await ffmpegResult.promise;
     } catch (err) {
       unregisterProcess(file.id);
       queries.updateFilePid.run(null, file.id);
       // Check if this was a cancellation
       const cancelled = queries.isJobCancelled.get(job.id);
       if (cancelled) {
         throw new Error('Job cancelled');
       }
       throw err; // Re-throw for non-cancellation errors
     }
     ```

2. **server/routes/jobs.js** -- Add POST /:id/cancel endpoint:
   ```js
   // POST /:id/cancel - Cancel a job
   router.post('/:id/cancel', requireAuth, async (req, res) => {
     const job = queries.getJob.get(req.params.id);

     if (!job) {
       return res.status(404).json({ error: 'Job not found' });
     }

     // Only queued or processing jobs can be cancelled
     if (job.status !== 'queued' && job.status !== 'processing') {
       return res.status(409).json({ error: `Cannot cancel job with status '${job.status}'` });
     }

     // Set job status to cancelled in DB FIRST (processor checks this)
     const result = queries.cancelJob.run(req.params.id);

     if (result.changes === 0) {
       // Race condition: job completed between our check and update
       const updatedJob = queries.getJob.get(req.params.id);
       return res.json({
         jobId: req.params.id,
         status: updatedJob.status,
         message: 'Job already completed'
       });
     }

     // If processing, kill FFmpeg processes
     if (job.status === 'processing') {
       const files = queries.getJobFiles.all(req.params.id);
       for (const file of files) {
         if (file.status === 'processing' && file.ffmpeg_pid) {
           await cancelJobProcesses(file.id);
           queries.clearFilePid.run(file.id);
         }
       }
     }

     // Get completion stats
     const files = queries.getJobFiles.all(req.params.id);
     const completedVariations = files.reduce((sum, f) => sum + (f.completed_variations || 0), 0);

     res.json({
       jobId: req.params.id,
       status: 'cancelled',
       completedVariations,
       totalVariations: job.total_variations
     });
   });
   ```
   Import `cancelJobProcesses` from `../lib/cancel.js` at the top of the file.

3. **server/routes/jobs.js** -- Update GET /:id endpoint to include `cancelledAt` in response when status is 'cancelled':
   After the existing response object (line 69-88), add `cancelledAt: job.cancelled_at || null` to the response JSON.

   Also update the download endpoint (GET /:id/download) to allow downloads for cancelled jobs that have output files. Change the status check from:
   ```js
   if (!job || job.status !== 'completed') {
   ```
   to:
   ```js
   if (!job || (job.status !== 'completed' && job.status !== 'cancelled')) {
   ```

4. **server/lib/queue.js** -- No changes needed. The queue worker already tracks `currentJobId` and the cancel endpoint handles killing FFmpeg directly. The processor's cancellation check between variations will cause the job to exit cleanly. The worker's catch block will handle the 'Job cancelled' error -- but actually it should NOT mark it as failed. Update the catch in poll():
   ```js
   } catch (err) {
     if (err.message !== 'Job cancelled') {
       console.error(`Job ${nextJob.id} failed:`, err.message);
       this.queries.updateJobError.run(err.message, nextJob.id);
     } else {
       console.log(`Job ${nextJob.id} was cancelled`);
     }
   }
   ```

5. **server/index.js** -- Update CORS allowed methods to include 'POST' (already present). No other changes needed. The gracefulShutdown function already handles killing FFmpeg by PID, which is fine.
  </action>
  <verify>
    Start the server with `cd /Users/alexkozhemiachenko/Downloads/Claude/video-refresher/server && node index.js` and verify it starts without errors. Then test the cancel endpoint:
    1. `curl -X POST http://localhost:8080/api/auth/login -H 'Content-Type: application/json' -d '{"password":"..."}' ` to get a token
    2. `curl -X POST http://localhost:8080/api/jobs/nonexistent/cancel -H 'Authorization: Bearer TOKEN'` should return 404
    3. Server should log startup messages without errors related to schema migration or query preparation

    Verify the module imports work: `node -e "import('./server/lib/cancel.js').then(() => console.log('OK'))"` and `node -e "import('./server/lib/processor.js').then(() => console.log('OK'))"` from project root.
  </verify>
  <done>
    - POST /api/jobs/:id/cancel endpoint exists and returns appropriate responses (404 for missing, 409 for wrong status, 200 with completion stats for success)
    - Processor checks for cancellation between variations and exits cleanly
    - Partial output files (on disk but not in DB) are cleaned up after cancellation
    - Queue worker does not mark cancelled jobs as failed
    - Download endpoint allows downloading completed variations from cancelled jobs
    - GET /api/jobs/:id includes cancelledAt field
  </done>
</task>

</tasks>

<verification>
1. Server starts without errors after schema migration adds cancelled_at column
2. POST /api/jobs/:id/cancel returns 404 for non-existent jobs
3. POST /api/jobs/:id/cancel returns 409 for already completed/failed jobs
4. FFmpeg is spawned with stdin piped (stdio: ['pipe', ...]) -- verify in ffmpeg.js
5. cancel.js module loads and exports expected functions
6. processor.js imports and uses registerProcess/unregisterProcess
7. queries.js includes cancelJob and isJobCancelled prepared statements
8. Cleanup daemon includes 'cancelled' in expiry and eviction queries
</verification>

<success_criteria>
- Cancel endpoint handles all job states correctly (queued, processing, completed, failed)
- FFmpeg graceful termination uses 3-stage escalation (stdin 'q' -> SIGTERM -> SIGKILL)
- Completed variations survive cancellation (only partial files cleaned up)
- Race condition handled: if job completes before kill, status stays 'completed'
- Server starts and runs without errors
</success_criteria>

<output>
After completion, create `.planning/phases/12-server-job-cancellation/12-01-SUMMARY.md`
</output>
