---
phase: 07-ffmpeg-processing-engine
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - server/index.js
  - server/routes/jobs.js
autonomous: true

must_haves:
  truths:
    - "Server starts the queue worker on boot and jobs begin processing automatically"
    - "SIGTERM/SIGINT triggers graceful shutdown: stops worker, kills active FFmpeg, marks interrupted job as failed"
    - "On startup, jobs stuck in 'processing' status are reset to 'failed' with a recovery message"
    - "Orphaned FFmpeg processes (pid stored in SQLite) are killed on startup"
    - "GET /api/jobs/:id response includes per-file progress_percent, completed_variations, error, and output files"
    - "Polling job status shows progress advancing from 0 to 100 during encoding"
  artifacts:
    - path: "server/index.js"
      provides: "Queue worker startup, graceful shutdown handlers, startup recovery"
      contains: "worker.start"
    - path: "server/routes/jobs.js"
      provides: "Enhanced job status response with progress, variations, outputs"
      contains: "progress_percent"
  key_links:
    - from: "server/index.js"
      to: "server/lib/queue.js"
      via: "imports and starts JobQueueWorker"
      pattern: "JobQueueWorker"
    - from: "server/index.js"
      to: "process signals"
      via: "SIGTERM/SIGINT handlers for graceful shutdown"
      pattern: "process\\.on\\('SIGTERM'"
    - from: "server/routes/jobs.js"
      to: "server/db/queries.js"
      via: "queries.getOutputFiles for variation output data"
      pattern: "getOutputFiles"
---

<objective>
Wire the queue worker into the server, add graceful shutdown with FFmpeg process cleanup, implement startup recovery for interrupted jobs, and enhance the job status endpoint with progress and output data.

Purpose: This plan turns the processing engine (Plans 01-02) into a production-ready system. Without startup recovery, crashed jobs silently disappear. Without graceful shutdown, FFmpeg processes become orphans. Without progress in the status endpoint, the frontend can't show encoding progress.

Output: Modified server/index.js with worker lifecycle and recovery, and enhanced routes/jobs.js with progress data.
</objective>

<execution_context>
@/Users/alexkozhemiachenko/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alexkozhemiachenko/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-ffmpeg-processing-engine/07-RESEARCH.md
@.planning/phases/07-ffmpeg-processing-engine/07-01-SUMMARY.md
@.planning/phases/07-ffmpeg-processing-engine/07-02-SUMMARY.md
@server/index.js
@server/routes/jobs.js
@server/lib/queue.js
@server/db/queries.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add startup recovery, graceful shutdown, and queue worker to server</name>
  <files>server/index.js</files>
  <action>
  Modify `server/index.js` to add three new capabilities:

  **1. Import and start queue worker:**
  ```javascript
  import { JobQueueWorker } from './lib/queue.js';
  ```
  After database initialization, create and start the worker:
  ```javascript
  const worker = new JobQueueWorker(db, queries, OUTPUT_DIR);
  ```
  Start the worker AFTER the server begins listening (inside the app.listen callback):
  ```javascript
  app.listen(PORT, () => {
    console.log(`Server listening on port ${PORT}`);
    // ... existing logs ...
    recoverInterruptedJobs(db, queries);
    worker.start();
  });
  ```

  **2. Startup recovery function (`recoverInterruptedJobs`):**
  Define this function in index.js (not a separate file -- it's small and only runs once):
  ```javascript
  function recoverInterruptedJobs(db, queries) {
    // 1. Find any jobs stuck in 'processing' status (server crashed mid-job)
    const stuckJobs = queries.getProcessingJobs.all();
    for (const job of stuckJobs) {
      console.log(`Recovery: marking interrupted job ${job.id} as failed`);
      queries.updateJobError.run('Server restarted during processing', job.id);

      // Mark any 'processing' files in this job as failed too
      const files = queries.getJobFiles.all(job.id);
      for (const file of files) {
        if (file.status === 'processing') {
          queries.updateFileError.run('Server restarted during processing', file.id);
        }
      }
    }

    // 2. Kill orphaned FFmpeg processes (pid stored in SQLite from previous run)
    const filesWithPid = queries.getFilesWithPid.all();
    for (const file of filesWithPid) {
      try {
        // Check if process is still alive
        process.kill(file.ffmpeg_pid, 0);
        // If we get here, process is alive -- kill it
        console.log(`Recovery: killing orphaned FFmpeg process ${file.ffmpeg_pid}`);
        process.kill(file.ffmpeg_pid, 'SIGKILL');
      } catch (err) {
        // ESRCH = process doesn't exist (already dead), which is fine
        if (err.code !== 'ESRCH') {
          console.error(`Recovery: error checking pid ${file.ffmpeg_pid}:`, err.message);
        }
      }
      // Clear the pid regardless
      queries.clearFilePid.run(file.id);
    }

    if (stuckJobs.length > 0 || filesWithPid.length > 0) {
      console.log(`Recovery: processed ${stuckJobs.length} stuck jobs, ${filesWithPid.length} orphaned processes`);
    }
  }
  ```

  **3. Graceful shutdown handlers:**
  Add SIGTERM and SIGINT handlers after the app.listen block:
  ```javascript
  async function gracefulShutdown(signal) {
    console.log(`${signal} received, shutting down gracefully...`);

    // 1. Stop the queue worker (prevents picking up new jobs)
    worker.stop();

    // 2. Kill any active FFmpeg processes
    const filesWithPid = queries.getFilesWithPid.all();
    for (const file of filesWithPid) {
      try {
        process.kill(file.ffmpeg_pid, 'SIGTERM');
        console.log(`Shutdown: sent SIGTERM to FFmpeg pid ${file.ffmpeg_pid}`);
      } catch (err) {
        if (err.code !== 'ESRCH') {
          console.error(`Shutdown: error killing pid ${file.ffmpeg_pid}:`, err.message);
        }
      }
      queries.clearFilePid.run(file.id);
    }

    // 3. Mark current job as failed if one was being processed
    const currentJobId = worker.getCurrentJobId();
    if (currentJobId) {
      try {
        queries.updateJobError.run(`Server shutdown (${signal})`, currentJobId);
        const files = queries.getJobFiles.all(currentJobId);
        for (const file of files) {
          if (file.status === 'processing') {
            queries.updateFileError.run(`Server shutdown (${signal})`, file.id);
          }
        }
      } catch (err) {
        console.error('Shutdown: error marking job failed:', err.message);
      }
    }

    // 4. Give FFmpeg processes 2 seconds to die, then SIGKILL
    await new Promise(r => setTimeout(r, 2000));
    const remainingPids = queries.getFilesWithPid.all();
    for (const file of remainingPids) {
      try {
        process.kill(file.ffmpeg_pid, 'SIGKILL');
      } catch (err) { /* ignore */ }
    }

    // 5. Close database
    db.close();
    console.log('Shutdown complete');
    process.exit(0);
  }

  process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
  process.on('SIGINT', () => gracefulShutdown('SIGINT'));
  ```

  Keep all existing code (CORS, routes, error handler) unchanged. The worker import and startup are additive.
  </action>
  <verify>
  `node -e "
    import fs from 'node:fs';
    const code = fs.readFileSync('./server/index.js', 'utf8');
    const checks = [
      ['JobQueueWorker import', /import.*JobQueueWorker/],
      ['worker.start()', /worker\\.start\\(\\)/],
      ['recoverInterruptedJobs', /recoverInterruptedJobs/],
      ['SIGTERM handler', /process\\.on\\('SIGTERM'/],
      ['SIGINT handler', /process\\.on\\('SIGINT'/],
      ['gracefulShutdown', /gracefulShutdown/],
    ];
    for (const [name, regex] of checks) {
      console.log(name + ':', regex.test(code) ? 'PASS' : 'FAIL');
    }
  "` -- all should print PASS.
  </verify>
  <done>
  Server starts queue worker on boot. Startup recovery marks stuck 'processing' jobs as failed and kills orphaned FFmpeg PIDs. SIGTERM/SIGINT handlers stop worker, kill active FFmpeg processes, mark interrupted job as failed, close database, then exit.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance job status endpoint with progress and output data</name>
  <files>server/routes/jobs.js</files>
  <action>
  Modify `server/routes/jobs.js` to enrich the GET /:id response with progress and output file information.

  The `createJobsRouter(db, queries)` function already receives db and queries. Update the GET /:id handler:

  **Current response per file:**
  ```json
  { "id": "...", "name": "...", "size": 123, "status": "queued" }
  ```

  **New response per file:**
  ```json
  {
    "id": "...",
    "name": "...",
    "size": 123,
    "status": "queued|processing|completed|failed",
    "progress": 0,
    "completedVariations": 0,
    "error": null,
    "outputs": []
  }
  ```

  Implementation:
  1. After fetching `files`, also fetch output files: `const outputFiles = queries.getOutputFiles.all(req.params.id)`
  2. Group output files by job_file_id for efficient lookup
  3. Map each file to include:
     - `progress`: `f.progress_percent || 0`
     - `completedVariations`: `f.completed_variations || 0`
     - `error`: `f.error || null`
     - `outputs`: array of output files for this file, each with `{ id, variationIndex, outputPath, fileSize }`

  Also enhance the top-level job response to include an `overallProgress` computed field:
  - `overallProgress`: average of all files' progress_percent, rounded to integer
  - If job status is 'completed': force overallProgress to 100
  - If job status is 'failed': leave as last recorded value

  Full updated GET /:id handler:
  ```javascript
  router.get('/:id', requireAuth, (req, res) => {
    const job = queries.getJob.get(req.params.id);
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }

    const files = queries.getJobFiles.all(req.params.id);
    const outputFiles = queries.getOutputFiles.all(req.params.id);

    // Group outputs by job_file_id
    const outputsByFile = {};
    for (const out of outputFiles) {
      if (!outputsByFile[out.job_file_id]) outputsByFile[out.job_file_id] = [];
      outputsByFile[out.job_file_id].push({
        id: out.id,
        variationIndex: out.variation_index,
        outputPath: out.output_path,
        fileSize: out.file_size
      });
    }

    // Compute overall progress
    let overallProgress = 0;
    if (files.length > 0) {
      const sum = files.reduce((acc, f) => acc + (f.progress_percent || 0), 0);
      overallProgress = Math.round(sum / files.length);
    }
    if (job.status === 'completed') overallProgress = 100;

    res.json({
      jobId: job.id,
      status: job.status,
      totalVideos: job.total_videos,
      totalVariations: job.total_variations,
      overallProgress,
      files: files.map(f => ({
        id: f.id,
        name: f.original_name,
        size: f.file_size,
        status: f.status,
        progress: f.progress_percent || 0,
        completedVariations: f.completed_variations || 0,
        error: f.error || null,
        outputs: outputsByFile[f.id] || []
      })),
      createdAt: job.created_at,
      expiresAt: job.expires_at,
      error: job.error
    });
  });
  ```

  Do NOT modify the POST / handler or the GET / (list) handler -- they remain as-is from Phase 6.
  </action>
  <verify>
  `node -e "
    import fs from 'node:fs';
    const code = fs.readFileSync('./server/routes/jobs.js', 'utf8');
    const checks = [
      ['getOutputFiles', /getOutputFiles/],
      ['overallProgress', /overallProgress/],
      ['progress_percent', /progress_percent/],
      ['completed_variations', /completed_variations/],
      ['outputsByFile', /outputsByFile/],
    ];
    for (const [name, regex] of checks) {
      console.log(name + ':', regex.test(code) ? 'PASS' : 'FAIL');
    }
  "` -- all should print PASS.

  Also verify the existing POST / handler is unchanged by checking it still contains `upload.array('videos', 10)`.
  </verify>
  <done>
  GET /api/jobs/:id returns per-file progress (0-100), completedVariations count, error message, and output file list. Overall job progress is computed as average of file progress percentages. POST / and GET / handlers unchanged from Phase 6.
  </done>
</task>

</tasks>

<verification>
1. Server starts without errors when index.js is loaded
2. Queue worker is started in app.listen callback
3. SIGTERM handler exists and calls worker.stop(), kills FFmpeg pids, closes db
4. Startup recovery marks 'processing' jobs as 'failed'
5. GET /api/jobs/:id response includes progress, completedVariations, outputs, overallProgress
6. POST /api/jobs and GET /api/jobs unchanged from Phase 6
</verification>

<success_criteria>
- Server boots, starts queue worker, and begins processing queued jobs automatically
- Graceful shutdown on SIGTERM: stops worker, kills FFmpeg, marks interrupted jobs failed, closes db
- Startup recovery: stuck 'processing' jobs reset to 'failed', orphaned FFmpeg pids killed
- Job status endpoint returns per-file progress percentage (0-100) and output file details
- All Phase 6 API behavior preserved (POST job creation, GET list)
</success_criteria>

<output>
After completion, create `.planning/phases/07-ffmpeg-processing-engine/07-03-SUMMARY.md`
</output>
