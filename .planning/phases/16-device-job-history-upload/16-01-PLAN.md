---
phase: 16-device-job-history-upload
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/db/schema.js
  - server/db/queries.js
  - server/routes/jobs.js
  - server/index.js
  - server/middleware/upload.js
autonomous: true

must_haves:
  truths:
    - "Server accepts pre-processed device results via POST /api/jobs/device"
    - "Uploaded blobs are stored in the output directory on disk"
    - "A job record is created with status 'completed' and source 'device'"
    - "job_files records are created with original source filenames"
    - "output_files records are created for each uploaded variation"
    - "The new endpoint requires authentication"
  artifacts:
    - path: "server/routes/jobs.js"
      provides: "POST /api/jobs/device endpoint"
      contains: "router.post.*device"
    - path: "server/db/schema.js"
      provides: "source column migration for jobs table"
      contains: "source"
    - path: "server/db/queries.js"
      provides: "insertDeviceJob query"
      contains: "insertDeviceJob"
  key_links:
    - from: "server/routes/jobs.js"
      to: "server/db/queries.js"
      via: "queries.insertDeviceJob"
      pattern: "queries\\.insertDeviceJob"
    - from: "server/routes/jobs.js"
      to: "data/output"
      via: "fs.writeFile to store uploaded blobs"
      pattern: "writeFile|createWriteStream"
---

<objective>
Create a server endpoint that accepts pre-processed device results and persists them as completed job records.

Purpose: Device-processed videos currently exist only as in-memory blobs in the browser. This endpoint allows the frontend to upload those results to the server so they appear in job history alongside server-processed jobs.

Output: New POST /api/jobs/device endpoint that receives multipart form data containing pre-processed MP4 blobs plus metadata, stores files to disk, and creates complete job records (jobs, job_files, output_files) with status='completed' and source='device'.
</objective>

<execution_context>
@/Users/alexkozhemiachenko/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alexkozhemiachenko/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@server/routes/jobs.js
@server/db/schema.js
@server/db/queries.js
@server/middleware/upload.js
@server/middleware/auth.js
@server/lib/id.js
@server/index.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add source column to jobs table and device job queries</name>
  <files>server/db/schema.js, server/db/queries.js</files>
  <action>
    In server/db/schema.js migrateSchema():
    - Add a migration for `jobs.source` column: `ALTER TABLE jobs ADD COLUMN source TEXT NOT NULL DEFAULT 'server'`
    - This distinguishes device-processed vs server-processed jobs. Existing jobs default to 'server'.

    In server/db/queries.js createJobQueries():
    - Add `insertDeviceJob` prepared statement that inserts a job with explicit source='device', status='completed', and expires_at = datetime('now', '+24 hours'). Fields: id, total_videos, total_variations, source, status, expires_at.
    - Add `insertDeviceJobFile` prepared statement that inserts into job_files with status='completed' and completed_variations set to the variation count. Fields: id, job_id, original_name, upload_path (empty string since no source upload), file_size (0 -- no source file stored), status, completed_variations.
    - Update the `listJobs` query to also SELECT the `source` column: add `source` to the SELECT (currently uses SELECT *, so it already gets it -- but verify this works and update the route response to include it).

    Do NOT modify any existing queries that work for server jobs.
  </action>
  <verify>
    Start the server locally (`cd server && node index.js`) and confirm it boots without SQLite errors. The migration should add the `source` column if not present.
  </verify>
  <done>
    - `source` column exists on jobs table with default 'server'
    - `insertDeviceJob` and `insertDeviceJobFile` queries are prepared and available
    - Existing server job flow is unaffected
  </done>
</task>

<task type="auto">
  <name>Task 2: Create POST /api/jobs/device endpoint</name>
  <files>server/routes/jobs.js, server/index.js</files>
  <action>
    In server/routes/jobs.js, add a new route inside createJobsRouter():

    `router.post('/device', requireAuth, upload.array('results', 200), async (req, res) => { ... })`

    The endpoint accepts:
    - `results` field: multipart file uploads containing pre-processed MP4 variation blobs
    - `sourceFiles` field (JSON string in body): array of objects `[{name: "video1.mp4", variationCount: 5}, {name: "video2.mov", variationCount: 5}]` describing the original source videos and how many variations each had
    - `variations` field: total variations per video (number)

    Implementation:
    1. Parse `sourceFiles` from `req.body.sourceFiles` (JSON.parse).
    2. Validate: sourceFiles must be non-empty array, each must have name (string) and variationCount (number).
    3. Validate: req.files must be non-empty.
    4. Generate a jobId via generateId().
    5. Calculate totalVideos = sourceFiles.length, totalVariations = req.files.length.
    6. Call queries.insertDeviceJob.run(jobId, totalVideos, totalVariations) to create the job.
    7. For each source file in sourceFiles, create a job_file record:
       - fileId = generateId()
       - queries.insertDeviceJobFile.run(fileId, jobId, sourceFile.name, '', 0, 'completed', sourceFile.variationCount)
    8. Move each uploaded result file from the multer tmp location to the OUTPUT_DIR (use fs.renameSync or fs.copyFileSync + unlinkSync).
       - The uploaded files arrive via multer in the UPLOAD_DIR. Move them to OUTPUT_DIR with a structured path.
       - For each uploaded file (req.files), determine which source file it belongs to by parsing the original filename pattern: the frontend will send files with originalname format `{sourceBaseName}/{sourceBaseName}_var{N}_{uuid}.mp4`. Extract the sourceBaseName from the first path segment.
       - Create output subdirectory: `{OUTPUT_DIR}/{jobId}/`
       - Move file to: `{OUTPUT_DIR}/{jobId}/{originalFilename}` (just the filename part, no subdirs)
       - Get file size from req.files[i].size
    9. For each moved file, insert an output_files record:
       - outputId = generateId()
       - Find the matching job_file_id by matching the sourceBaseName to the job_file's original_name (strip extension for comparison)
       - queries.insertOutputFile.run(outputId, jobId, matchingJobFileId, variationIndex, outputPath, fileSize)
       - Parse variationIndex from the filename (the `var{N}` part)
    10. Return 201 with: { jobId, status: 'completed', totalVideos, totalVariations, source: 'device' }

    Important details:
    - Create a second multer instance in middleware/upload.js: export `deviceUpload` that reuses the same storage but has NO fileFilter and a higher files limit (200). This is needed because browser-created Blobs may have a generic MIME type, and these are pre-processed results not raw uploads.
    - The OUTPUT_DIR path needs to be available in the route. Pass it via the createJobsRouter function: `createJobsRouter(db, queries, outputDir)` -- update server/index.js to pass OUTPUT_DIR.
    - Wrap DB operations in a transaction for atomicity: `db.transaction(() => { ... })()`.
    - Create the job output directory `{OUTPUT_DIR}/{jobId}/` before moving files.

    In server/index.js:
    - Update the createJobsRouter call to pass OUTPUT_DIR: `createJobsRouter(db, queries, OUTPUT_DIR)`

    In server/middleware/upload.js:
    - Export a second multer instance `deviceUpload` that uses the same storage but no fileFilter, and a higher files limit (200).
  </action>
  <verify>
    Test with curl:
    ```
    # Get auth token first
    TOKEN=$(curl -s -X POST http://localhost:8080/api/auth/login -H 'Content-Type: application/json' -d '{"password":"test"}' | jq -r '.token')

    # Create a small test MP4 file (or use any mp4)
    # Upload device results
    curl -X POST http://localhost:8080/api/jobs/device \
      -H "Authorization: Bearer $TOKEN" \
      -F 'sourceFiles=[{"name":"test.mp4","variationCount":1}]' \
      -F "results=@/path/to/test.mp4;filename=test/test_var1_abc12345.mp4" \
      -v
    ```
    Expect 201 response with jobId. Then verify:
    ```
    curl http://localhost:8080/api/jobs -H "Authorization: Bearer $TOKEN" | jq
    ```
    The new job should appear with source: 'device' (or at minimum, be listed).
  </verify>
  <done>
    - POST /api/jobs/device endpoint exists and requires auth
    - Accepts multipart pre-processed result files + sourceFiles metadata
    - Creates job record with source='device' and status='completed'
    - Creates job_files and output_files records
    - Files stored in OUTPUT_DIR/{jobId}/
    - Job appears in GET /api/jobs listing
    - GET /api/jobs response includes source field for all jobs
  </done>
</task>

</tasks>

<verification>
1. Server starts without errors after schema migration
2. POST /api/jobs/device returns 201 with job details
3. GET /api/jobs lists the device job alongside any existing server jobs
4. GET /api/jobs/:id returns full job detail including files and output_files
5. GET /api/jobs/:id/download returns a ZIP of the device-processed results
6. Existing POST /api/jobs (server upload) still works unchanged
</verification>

<success_criteria>
- New endpoint POST /api/jobs/device is functional and authenticated
- Device jobs are stored in database with source='device', status='completed'
- Output files are persisted to disk in OUTPUT_DIR
- Job listing includes source field to distinguish device vs server jobs
- Existing server job workflow is completely unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/16-device-job-history-upload/16-01-SUMMARY.md`
</output>
